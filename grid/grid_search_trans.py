import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader
from transformer_model import TransformerForecast
from dataset.power_dataset import PowerDataset
import pandas as pd
import matplotlib.pyplot as plt
import os
import random
import joblib
from itertools import product

# åŠ è½½ç›®æ ‡åˆ—çš„ scaler
target_scaler = joblib.load('scalers/target_scaler.pkl')

# å›ºå®šéšæœºç§å­
def set_seed(seed=42):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    if hasattr(torch, 'set_float32_matmul_precision'):
        torch.set_float32_matmul_precision('high')

# ==== é…ç½® ====
INPUT_LEN = 90
OUTPUT_LEN = 90
EPOCHS = 500
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
N_REPEATS = 5
PATIENCE = 10  # Early stopping patience

# ==== Transformer ç½‘æ ¼æœç´¢å‚æ•° ====
HYPERPARAMS = {
    'batch_size': [8, 16, 32],
    'd_model': [16, 32, 64],       # æ¨¡å‹ç»´åº¦
    'nhead': [2, 4, 8],            # æ³¨æ„åŠ›å¤´æ•°
    'num_layers': [1, 2, 3],       # Transformer å±‚æ•°
    'dropout': [0.1, 0.2, 0.3],   # Dropout ç‡
    'lr': [1e-5, 5e-5, 1e-4]      # å­¦ä¹ ç‡
}

def plot_prediction_curve(y_true, y_pred, output_len, save_path=None, title="Prediction vs Ground Truth"):
    days = np.arange(output_len)
    plt.figure(figsize=(12, 6))
    plt.plot(days, y_true, label='Ground Truth')
    plt.plot(days, y_pred, label='Prediction')
    plt.xlabel("Future Days")
    plt.ylabel("Normalized Global_active_power(KW)")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300)
        print(f"[+] å›¾åƒå·²ä¿å­˜è‡³ {save_path}")
    else:
        plt.show()

def plot_loss_curves(train_losses, test_losses, save_path):
    plt.figure(figsize=(8, 4))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(test_losses, label='Test Loss')
    plt.xlabel("Epoch")
    plt.ylabel("SmoothL1Loss")
    plt.title("Train vs Test Loss Curve")
    plt.legend()
    plt.grid(True)
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300)
    print(f"[+] Loss æ›²çº¿å›¾å·²ä¿å­˜è‡³ {save_path}")
    plt.close()

# ==== EarlyStopping ç±» ====
class EarlyStopping:
    def __init__(self, patience=10, delta=1e-4):
        self.patience = patience
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.delta = delta

    def step(self, metric):
        if self.best_score is None or metric < self.best_score - self.delta:
            self.best_score = metric
            self.counter = 0
        else:
            self.counter += 1
        if self.counter >= self.patience:
            self.early_stop = True
        return self.early_stop

# ==== è®­ç»ƒå‡½æ•° ====
def train_one_epoch(model, data_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    total_samples = 0
    
    for inputs, targets in data_loader:
        inputs = inputs.to(device)
        targets = targets.to(device)
        
        optimizer.zero_grad()
        
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * inputs.size(0)
        total_samples += inputs.size(0)
    
    return total_loss / total_samples

# ==== è¯„ä¼°å‡½æ•° ====
def evaluate(model, data_loader, device, criterion=None):
    model.eval()
    all_preds = []
    all_targets = []
    total_smoothl1 = 0.0
    total_mse = 0.0
    total_mae = 0.0
    total_samples = 0
    
    with torch.no_grad():
        for inputs, targets in data_loader:
            inputs = inputs.to(device)
            targets = targets.to(device)
            
            outputs = model(inputs)
            
            # è®¡ç®—SmoothL1Loss
            if criterion:
                smoothl1_loss = criterion(outputs, targets)
                total_smoothl1 += smoothl1_loss.item() * inputs.size(0)
            
            # è®¡ç®—MSEå’ŒMAE
            mse = F.mse_loss(outputs, targets).item()
            mae = F.l1_loss(outputs, targets).item()
            
            total_mse += mse * inputs.size(0)
            total_mae += mae * inputs.size(0)
            total_samples += inputs.size(0)
            
            all_preds.append(outputs.cpu().numpy())
            all_targets.append(targets.cpu().numpy())
    
    # è®¡ç®—å¹³å‡æŸå¤±
    avg_smoothl1 = total_smoothl1 / total_samples if criterion else None
    avg_mse = total_mse / total_samples
    avg_mae = total_mae / total_samples
    
    all_preds = np.concatenate(all_preds, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    return avg_smoothl1, avg_mse, avg_mae, all_preds, all_targets

# ==== åŠ è½½æ•°æ® ====
train_df = pd.read_csv('data/cleaned2_train.csv', index_col=0)
test_df = pd.read_csv('data/cleaned2_test.csv', index_col=0)

train_dataset = PowerDataset(train_df, input_len=INPUT_LEN, output_len=OUTPUT_LEN)
test_dataset = PowerDataset(test_df, input_len=INPUT_LEN, output_len=OUTPUT_LEN)

# ==== ç½‘æ ¼æœç´¢ ====
best_params = None
best_score = float('inf')
results = []

# ç”Ÿæˆæ‰€æœ‰è¶…å‚æ•°ç»„åˆ
param_combinations = list(product(*HYPERPARAMS.values()))
param_names = list(HYPERPARAMS.keys())

print(f"å¼€å§‹Transformerç½‘æ ¼æœç´¢ï¼Œå…± {len(param_combinations)} ç»„å‚æ•°ç»„åˆ...")

for i, params in enumerate(param_combinations):
    param_dict = dict(zip(param_names, params))
    print(f"\n=== æµ‹è¯•å‚æ•°ç»„åˆ {i+1}/{len(param_combinations)}: {param_dict} ===")
    
    # è®°å½•å½“å‰å‚æ•°ç»„åˆçš„æ€§èƒ½
    param_smoothl1 = []
    param_mse = []
    param_mae = []
    
    for repeat in range(N_REPEATS):
        print(f"\nğŸš€ ç¬¬ {repeat+1}/{N_REPEATS} è½®è®­ç»ƒ")
        
        seed = 42 + repeat
        set_seed(seed)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä½¿ç”¨å½“å‰æ‰¹å¤§å°
        train_loader = DataLoader(train_dataset, 
                                 batch_size=param_dict['batch_size'], 
                                 shuffle=True,
                                 worker_init_fn=lambda id: set_seed(seed + id))
        
        test_loader = DataLoader(test_dataset, 
                                batch_size=param_dict['batch_size'], 
                                shuffle=False)
        
        # åˆ›å»ºTransformeræ¨¡å‹
        model = TransformerForecast(
            input_dim=13,
            d_model=param_dict['d_model'],
            nhead=param_dict['nhead'],
            num_layers=param_dict['num_layers'],
            output_len=OUTPUT_LEN,
            dropout=param_dict['dropout']
        ).to(DEVICE)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(model.parameters(), lr=param_dict['lr'], weight_decay=1e-5)
        criterion = nn.SmoothL1Loss()
        early_stopper = EarlyStopping(patience=PATIENCE)
        
        best_test_smoothl1 = float('inf')
        best_model_path = f"checkpoints/transformer_best_model_param_{i}_repeat_{repeat}.pth"
        os.makedirs("checkpoints", exist_ok=True)
        
        train_losses = []
        test_smoothl1_losses = []
        
        for epoch in range(EPOCHS):
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)
            
            # è¯„ä¼°æ¨¡å‹
            test_smoothl1, test_mse, test_mae, _, _ = evaluate(
                model, test_loader, DEVICE, criterion
            )
            
            train_losses.append(train_loss)
            test_smoothl1_losses.append(test_smoothl1)
            
            # æ ¹æ®SmoothL1Lossä¿å­˜æ¨¡å‹
            if test_smoothl1 < best_test_smoothl1:
                best_test_smoothl1 = test_smoothl1
                torch.save(model.state_dict(), best_model_path)
            
            # æ—©åœåŸºäºSmoothL1Loss
            if early_stopper.step(test_smoothl1):
                break
        
        # åŠ è½½æœ€ä¼˜æ¨¡å‹å¹¶è¯„ä¼°
        model.load_state_dict(torch.load(best_model_path))
        test_smoothl1, test_mse, test_mae, _, _ = evaluate(
            model, test_loader, DEVICE, criterion
        )
        
        param_smoothl1.append(test_smoothl1)
        param_mse.append(test_mse)
        param_mae.append(test_mae)
        
        print(f"å‚æ•°ç»„åˆ {param_dict} - ç¬¬ {repeat+1} è½®: SmoothL1Loss={test_smoothl1:.4f}, MSE={test_mse:.4f}, MAE={test_mae:.4f}")
    
    # è®¡ç®—å½“å‰å‚æ•°ç»„åˆçš„å¹³å‡æ€§èƒ½
    avg_smoothl1 = np.mean(param_smoothl1)
    avg_mse = np.mean(param_mse)
    avg_mae = np.mean(param_mae)
    
    results.append({
        'params': param_dict,
        'avg_smoothl1': avg_smoothl1,
        'avg_mse': avg_mse,
        'avg_mae': avg_mae,
        'all_smoothl1': param_smoothl1,
        'all_mse': param_mse,
        'all_mae': param_mae
    })
    
    print(f"å‚æ•°ç»„åˆ {param_dict} å¹³å‡æ€§èƒ½: SmoothL1Loss={avg_smoothl1:.4f}, MSE={avg_mse:.4f}, MAE={avg_mae:.4f}")
    
    # æ›´æ–°æœ€ä½³å‚æ•°
    if avg_smoothl1 < best_score:
        best_score = avg_smoothl1
        best_params = param_dict
        print(f"ğŸ”¥ æ–°æœ€ä½³å‚æ•°ç»„åˆ: {best_params} (SmoothL1Loss={best_score:.4f})")

# ==== ä½¿ç”¨æœ€ä½³å‚æ•°è¿›è¡Œæœ€ç»ˆè®­ç»ƒå’Œè¯„ä¼° ====
print(f"\næœ€ä½³å‚æ•°ç»„åˆ: {best_params}")
print(f"æœ€ä½³SmoothL1Loss: {best_score:.4f}")

# ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
results_df = pd.DataFrame(results)
results_df.to_csv('transformer_grid_search_results.csv', index=False)
print("Transformerç½‘æ ¼æœç´¢ç»“æœå·²ä¿å­˜è‡³ transformer_grid_search_results.csv")

# ä½¿ç”¨æœ€ä½³å‚æ•°è¿›è¡Œæœ€ç»ˆè®­ç»ƒ
all_smoothl1, all_mse, all_mae = [], [], []
all_preds_inv_all_exps = []
all_targets_inv_all_exps = []

for i in range(N_REPEATS):
    print(f"\nğŸš€ æœ€ç»ˆè®­ç»ƒ - ç¬¬ {i+1}/{N_REPEATS} è½®")
    
    seed = 42 + i
    set_seed(seed)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä½¿ç”¨æœ€ä½³æ‰¹å¤§å°
    train_loader = DataLoader(train_dataset, 
                             batch_size=best_params['batch_size'], 
                             shuffle=True,
                             worker_init_fn=lambda id: set_seed(seed + id))
    
    test_loader = DataLoader(test_dataset, 
                            batch_size=best_params['batch_size'], 
                            shuffle=False)
    
    # åˆ›å»ºTransformeræ¨¡å‹
    model = TransformerForecast(
        input_dim=13,
        d_model=best_params['d_model'],
        nhead=best_params['nhead'],
        num_layers=best_params['num_layers'],
        output_len=OUTPUT_LEN,
        dropout=best_params['dropout']
    ).to(DEVICE)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['lr'], weight_decay=1e-5)
    criterion = nn.SmoothL1Loss()
    early_stopper = EarlyStopping(patience=PATIENCE)
    
    best_test_smoothl1 = float('inf')
    best_model_path = f"checkpoints/best_transformer_final_model_exp{i+1}.pth"
    
    train_losses = []
    test_smoothl1_losses = []
    
    for epoch in range(EPOCHS):
        # è®­ç»ƒä¸€ä¸ªepoch
        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)
        
        # è¯„ä¼°æ¨¡å‹
        test_smoothl1, test_mse, test_mae, _, _ = evaluate(
            model, test_loader, DEVICE, criterion
        )
        
        train_losses.append(train_loss)
        test_smoothl1_losses.append(test_smoothl1)
        
        # æ ¹æ®SmoothL1Lossä¿å­˜æ¨¡å‹
        if test_smoothl1 < best_test_smoothl1:
            best_test_smoothl1 = test_smoothl1
            torch.save(model.state_dict(), best_model_path)
            print(f"âœ… ä¿å­˜æµ‹è¯•é›†æœ€ä¼˜æ¨¡å‹ï¼ˆSmoothL1Loss={test_smoothl1:.4f}ï¼‰ï¼š{best_model_path}")
        
        # æ—©åœåŸºäºSmoothL1Loss
        if early_stopper.step(test_smoothl1):
            print(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
            break
    
    # åŠ è½½æœ€ä¼˜æ¨¡å‹å¹¶è¯„ä¼°
    model.load_state_dict(torch.load(best_model_path))
    test_smoothl1, test_mse, test_mae, all_preds, all_targets = evaluate(
        model, test_loader, DEVICE, criterion
    )
    
    all_smoothl1.append(test_smoothl1)
    all_mse.append(test_mse)
    all_mae.append(test_mae)
    
    print(f"\nğŸ“ˆ æµ‹è¯•é›†æœ€ç»ˆæ€§èƒ½ï¼šSmoothL1Loss = {test_smoothl1:.4f}, "
          f"MSE = {test_mse:.4f}, MAE = {test_mae:.4f}")
    
    # å¯¹é¢„æµ‹å€¼å’ŒçœŸå®å€¼è¿›è¡Œé€†å½’ä¸€åŒ–
    all_preds_inv = target_scaler.inverse_transform(all_preds.reshape(-1, 1)).reshape(all_preds.shape)
    all_targets_inv = target_scaler.inverse_transform(all_targets.reshape(-1, 1)).reshape(all_targets.shape)
    
    # æ”¶é›†åå½’ä¸€åŒ–ç»“æœ
    all_preds_inv_all_exps.append(all_preds_inv)
    all_targets_inv_all_exps.append(all_targets_inv)
    
    # ==== å¯è§†åŒ–éƒ¨åˆ† ====
    
    # å¯è§†åŒ–é¢„æµ‹
    sample_id = 0
    plot_prediction_curve(
        all_targets_inv.reshape(-1, OUTPUT_LEN)[sample_id],
        all_preds_inv.reshape(-1, OUTPUT_LEN)[sample_id],
        output_len=OUTPUT_LEN,
        save_path=f"plots_transformer_best/pred_vs_true_len{OUTPUT_LEN}_exp{i+1}_sample{sample_id}.png",
        title=f"Transformer Prediction vs Ground Truth (Sample {sample_id}) [Inverse]"
    )
    
    # å¯è§†åŒ–Lossæ›²çº¿
    plot_loss_curves(
        train_losses,
        test_smoothl1_losses,
        save_path=f"plots_transformer_best/loss_curve_exp{i+1}.png"
    )

# ==== æœ€ç»ˆç»“æœ ====
print("\nğŸ“Š 5è½®å¹³å‡æŒ‡æ ‡ï¼š")
print(f"SmoothL1Loss: {np.mean(all_smoothl1):.4f} Â± {np.std(all_smoothl1):.4f}")
print(f"MSE: {np.mean(all_mse):.4f} Â± {np.std(all_mse):.4f}")
print(f"MAE: {np.mean(all_mae):.4f} Â± {np.std(all_mae):.4f}")

# è®¡ç®—å¹³å‡è¯¯å·®æœ€å°çš„æ ·æœ¬
print("\nğŸ¯ æ­£åœ¨è®¡ç®—äº”è½®å¹³å‡è¯¯å·®æœ€å°çš„æ ·æœ¬...")

all_preds_array = np.stack(all_preds_inv_all_exps, axis=0)
all_targets_array = np.stack(all_targets_inv_all_exps, axis=0)

mean_preds = np.mean(all_preds_array, axis=0)
mean_targets = np.mean(all_targets_array, axis=0)

mse_per_sample = np.mean((mean_preds - mean_targets) ** 2, axis=1)
best_sample_id = np.argmin(mse_per_sample)

print(f"âœ… å…¨å±€æœ€ä¼˜æ ·æœ¬ ID: {best_sample_id}")

# ç»˜åˆ¶è¯¥æ ·æœ¬çš„å¹³å‡é¢„æµ‹æ•ˆæœå›¾
plot_prediction_curve(
    mean_targets[best_sample_id],
    mean_preds[best_sample_id],
    output_len=OUTPUT_LEN,
    save_path=f"plots_transformer_best/avg_prediction_sample{best_sample_id}.png",
    title=f"Transformer Avg Prediction over 5 Runs (Sample {best_sample_id})"
)

# ç»˜åˆ¶è¯¥æ ·æœ¬åœ¨æ¯ä¸€è½®çš„é¢„æµ‹æ•ˆæœå›¾
print(f"\nğŸ–¼ æ­£åœ¨è¾“å‡º sample_id={best_sample_id} åœ¨æ¯è½®çš„é¢„æµ‹å›¾...")

for i in range(N_REPEATS):
    y_true = all_targets_array[i, best_sample_id]
    y_pred = all_preds_array[i, best_sample_id]
    
    plot_prediction_curve(
        y_true,
        y_pred,
        output_len=OUTPUT_LEN,
        save_path=f"plots_transformer_best/best_sample{best_sample_id}_exp{i+1}.png",
        title=f"Transformer Sample {best_sample_id} - Prediction in Run {i+1}"
    )